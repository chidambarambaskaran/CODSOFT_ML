{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN5mhjN9R0VxDCA3v/+Y/jm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YS6JmKryjn0P","executionInfo":{"status":"ok","timestamp":1721201001797,"user_tz":-330,"elapsed":23568,"user":{"displayName":"CHIDAMBARAM","userId":"18166549547581218910"}},"outputId":"46fcbe34-245f-4cce-e058-26649e4317f7"},"outputs":[{"output_type":"stream","name":"stdout","text":["   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n","0          1    15634602  Hargrave          619    France  Female   42   \n","1          2    15647311      Hill          608     Spain  Female   41   \n","2          3    15619304      Onio          502    France  Female   42   \n","3          4    15701354      Boni          699    France  Female   39   \n","4          5    15737888  Mitchell          850     Spain  Female   43   \n","\n","   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n","0       2       0.00              1          1               1   \n","1       1   83807.86              1          0               1   \n","2       8  159660.80              3          1               0   \n","3       1       0.00              2          0               0   \n","4       2  125510.82              1          1               1   \n","\n","   EstimatedSalary  Exited  \n","0        101348.88       1  \n","1        112542.58       0  \n","2        113931.57       1  \n","3         93826.63       0  \n","4         79084.10       0  \n","Logistic Regression Classifier\n","Accuracy: 0.815\n","              precision    recall  f1-score   support\n","\n","           0       0.83      0.97      0.89      1607\n","           1       0.60      0.18      0.28       393\n","\n","    accuracy                           0.81      2000\n","   macro avg       0.71      0.58      0.59      2000\n","weighted avg       0.78      0.81      0.77      2000\n","\n","\n","Random Forest Classifier\n","Accuracy: 0.8705\n","              precision    recall  f1-score   support\n","\n","           0       0.88      0.97      0.92      1607\n","           1       0.78      0.47      0.59       393\n","\n","    accuracy                           0.87      2000\n","   macro avg       0.83      0.72      0.76      2000\n","weighted avg       0.86      0.87      0.86      2000\n","\n","\n","Gradient Boosting Classifier\n","Accuracy: 0.8655\n","              precision    recall  f1-score   support\n","\n","           0       0.88      0.96      0.92      1607\n","           1       0.75      0.47      0.58       393\n","\n","    accuracy                           0.87      2000\n","   macro avg       0.82      0.72      0.75      2000\n","weighted avg       0.86      0.87      0.85      2000\n","\n"]}],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","# Loading the dataset\n","file_path = 'Churn_Modelling.csv'\n","data = pd.read_csv(file_path)\n","\n","# Displaying the first few rows of the dataset\n","print(data.head())\n","\n","# Preprocessing the data\n","# Dropping the  unnecessary columns\n","data = data.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)\n","\n","# Encoding categorical variables\n","label_encoder = LabelEncoder()\n","data['Geography'] = label_encoder.fit_transform(data['Geography'])\n","data['Gender'] = label_encoder.fit_transform(data['Gender'])\n","\n","# Defining features and target variable\n","X = data.drop('Exited', axis=1)\n","y = data['Exited']\n","\n","# Splitting the data into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Standardizing the features\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","\n","# Logistic Regression\n","lr_model = LogisticRegression()\n","lr_model.fit(X_train, y_train)\n","lr_predictions = lr_model.predict(X_test)\n","print(\"Logistic Regression Classifier\")\n","print(\"Accuracy:\", accuracy_score(y_test, lr_predictions))\n","print(classification_report(y_test, lr_predictions))\n","\n","# Random Forest\n","rf_model = RandomForestClassifier()\n","rf_model.fit(X_train, y_train)\n","rf_predictions = rf_model.predict(X_test)\n","print(\"\\nRandom Forest Classifier\")\n","print(\"Accuracy:\", accuracy_score(y_test, rf_predictions))\n","print(classification_report(y_test, rf_predictions))\n","\n","# Gradient Boosting\n","gb_model = GradientBoostingClassifier()\n","gb_model.fit(X_train, y_train)\n","gb_predictions = gb_model.predict(X_test)\n","print(\"\\nGradient Boosting Classifier\")\n","print(\"Accuracy:\", accuracy_score(y_test, gb_predictions))\n","print(classification_report(y_test, gb_predictions))\n"]}]}